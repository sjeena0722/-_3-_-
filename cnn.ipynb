{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e4ffeb-4e80-478b-80e6-0a89e887c5cf",
   "metadata": {},
   "source": [
    "## CNN 모델\n",
    "* 컨볼루션 -> 풀링 -> 컨볼루션 -> 드롭아웃 -> 풀링 -> 신경망 -> 드롭아웃 -> 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e85c2d-9d82-4f57-80b4-7ffdf1e3e3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6190bfc6-fdc0-441c-883b-73f9f99a2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # 계산을 가속해주는 CUDA를 사용할 수 있는지 확인\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # USE_CUDA가 True이면 gpu, False이면 cpu로 보내도록 가리키는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128207b2-edf4-4cc3-bd03-af3afb52bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에폭과 배치크기를 정함\n",
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e5cf6-67b3-4e34-b38a-e60f523912f6",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a259dc9f-a721-4cc1-a63c-b2b0eccefc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac1cb7b41e943d7915896b59910fa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bba754f9ddd4f009af63b49bfe9b4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd299242d9a49c9ab555be38580c141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fac7428526e489eb00b2c3c62721857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST 데이터셋 불러오기\n",
    "# 60,000개의 학습 예제와 10,000개의 테스트 예제\n",
    "# 흑백(grayscale)의 28x28 이미지와 10개 분류(class) 중 하나인 정답(label)으로 구성\n",
    "\n",
    "\n",
    "# Train data loader\n",
    "train_loader = torch.utils.data.DataLoader( # DataLoader : 데이터셋을 batch 단위로 쪼개서 학습할 때 모델의 입력으로 주는 클래스\n",
    "    datasets.MNIST('./.data', #학습/테스트 데이터가 저장되는 경로\n",
    "                   train=True, # 학습용 또는 테스트용 데이터셋 여부를 지정\n",
    "                   download=True, #  root 에 데이터가 없는 경우 인터넷에서 다운로드\n",
    "                   transform=transforms.Compose([   # torchvision의 transform package를 사용하면 DataLoader에 직접 torchvistion dataset을 넣어주고 간결하게 재구성이 가능함 (입력 변환 가능)\n",
    "                       # 학습을 하려면 정규화(normalize)된 텐서 형태의 특징(feature)이 필요\n",
    "                       transforms.ToTensor(), # 이미지를 텐서로 변환\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) # 이미지 정규화\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test data loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./.data',\n",
    "                   train=False, # 테스트 데이터\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4cb1e-a989-40fb-ad20-1dd9ddbb7c0c",
   "metadata": {},
   "source": [
    "* ToTensor() : PIL Image나 NumPy ndarray 를 FloatTensor로 변환하고, 이미지의 픽셀의 크기 값을 [0., 1.] 범위로 비례하여 조정(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7dc49-2d7e-47f0-899d-ce0d14d0b2a3",
   "metadata": {},
   "source": [
    "## 뉴럴넷으로 Fashion MNIST 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0de41-b8f2-42f2-bca1-1d0e1091bb17",
   "metadata": {},
   "source": [
    "* CNN 클래스를 구현하는 init함수와 실제 데이터가 지나가는 forward 함수로 나뉨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6645542f-470d-4f8f-bfab-4b87f15a995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # 필터 : 5x5 , 숫자를 하나만 지정해도 정사각형으로 간주함\n",
    "        # 첫번째 파라미터 : 입력 채널 수 (in_channels) => 흑백 이미지이므로 1\n",
    "        # 두번째 파라미터 : 출력 채널 수 (out_channel) => 필터의 개수가 10개 => 10개의 특징맵을 생성\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # 입력 채널 수 : 10 (conv1의 결과물)\n",
    "        # 출력 채널 수 : 20 => 필터의 개수가 20개 => 20개의 특징맵을 생성\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # Dropout 적용 (Default: 0.5)\n",
    "        # 모델에 규제를 주어서 overfitting을 방지\n",
    "        # 무작위로 일부 뉴런을 생략시킴\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        # 일반 신경망을 거치면서 이전 출력 크기인 320에서 50,10순으로 작아지도록 함\n",
    "        # 50은 중간값으로 임의로 지정한 것이고, 10은 분류해야 할 클래스의 개수 (10개)를 의미\n",
    "\n",
    "    # 실제 데이터가 지나가는 길을 나타내는 함수    \n",
    "    def forward(self, x):\n",
    "        # 각 레이어는 conv-max pooling - ReLU를 하나의 묶음으로 간주\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) # max pooling 2x2 kernal을 의미            \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) # 중간에 dropout 진행\n",
    "        \n",
    "        x = x.view(-1, 320) # 컨볼루션 계층 2개를 거쳐 특징맵이 된 x를 FC layer에 넣기 위해서 2차원에서 1차원으로 펴기 (-1은 남는 차원 모두, 320은 x가 가진 원소개수)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.dropout(x, training=self.training) # ReLU 활성화 함수를 거친 뒤 드롭아웃을 사용\n",
    "        x = self.fc2(x) # 0부터 9까지 레이블을 갖는 10개의 출력값 생성\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0395f67-234b-4ab9-b196-7950da23dc28",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터\n",
    "to() 함수는 모델의 파라미터들을 지정한 곳으로 보내는 역할을 합니다. 일반적으로 CPU 1개만 사용할 경우 필요는 없지만, GPU를 사용하고자 하는 경우 to(\"cuda\")로 지정하여 GPU로 보내야 합니다. 지정하지 않을 경우 계속 CPU에 남아 있게 되며 빠른 훈련의 이점을 누리실 수 없습니다.\n",
    "\n",
    "최적화 알고리즘으로 파이토치에 내장되어 있는 optim.SGD를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1109bd-a2b8-4dc6-bc6c-aa2f5ce25041",
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Net().to(DEVICE) # 방금 만든 Net 모델 (CNN)의 인스턴스 생성\n",
    "# to() 함수 : 모델의 파라미터들을 지정한 장치의 메모리로 보내는 역할\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # 최적화 알고리즘으로 파이토치에 내장되어 있는 optim.SGD(확률적 경사하강법)를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f42022-d807-437d-b726-9203585bd9a0",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252de271-18a2-4518-8724-da58fb329338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 코드\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        # 학습데이터를 device의 메모리로 보냄\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        # 한 번 학습이 완료 되면 기울기를 새로 계산하므로 0을 만들어 줘야됨\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()# backpropagation을 통해 gradient 계산 => 각 매개변수에 대한 기울기 저장\n",
    "        optimizer.step() # 역전파 단계에서 수집된 기울기로 가중치 수정\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5adba-5090-472d-8e5f-8d56ce3236a2",
   "metadata": {},
   "source": [
    "## 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc08022d-e499-4de4-8cff-ee53767b3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval() # 해당 모델의 모든 레이어가 evalution mode에 들어가게 함 => 학습할 때만 필요한 Dropout, Batchnorm등의 기능을 비활성화 시킴\n",
    "    test_loss = 0\n",
    "    correct = 0 # 정답개수\n",
    "    with torch.no_grad(): # autograde(gradient를 계산해주는 것)를 비활성화 시킴 => 필요한 메모리가 줄어들고 연산 속도 증가\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item() # 미니 배치의 오차 평균 대신 합을 받아옴\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1] \n",
    "            # 출력되는 max 값은 값과 인덱스 => 레이블에 해당하는 인덱스가 필요하므로 두번쨰 원소를 불러와야됨 => 1\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # target.view_as(pred) : target을 pred의 shape에 맞춰 비교\n",
    "            # target과 pred가 일치하면 correct에 1을 더함\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae8b0-3a52-49c7-97b1-f20a8d7cd25b",
   "metadata": {},
   "source": [
    "## 코드 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b367c70b-5018-4463-a5ff-030c3430f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.343865\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.213240\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.311867\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.361549\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.570619\n",
      "[1] Test Loss: 0.1668, Accuracy: 94.88%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.347148\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.564691\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.547083\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.426779\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.250864\n",
      "[2] Test Loss: 0.1021, Accuracy: 96.87%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.352621\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.211621\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.479466\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.353841\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.262848\n",
      "[3] Test Loss: 0.0866, Accuracy: 97.35%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.331998\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.362118\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.350231\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.143018\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.316938\n",
      "[4] Test Loss: 0.0714, Accuracy: 97.80%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.155211\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.282303\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.111996\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.177619\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.198716\n",
      "[5] Test Loss: 0.0651, Accuracy: 97.98%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.165756\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.301308\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.182835\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.223792\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.076002\n",
      "[6] Test Loss: 0.0598, Accuracy: 98.28%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.175501\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.255099\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.120795\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.270595\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.203559\n",
      "[7] Test Loss: 0.0546, Accuracy: 98.36%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.091696\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.199140\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.158688\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.247199\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.099153\n",
      "[8] Test Loss: 0.0494, Accuracy: 98.53%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.088581\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.170777\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.104011\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.370185\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.108483\n",
      "[9] Test Loss: 0.0502, Accuracy: 98.44%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.205499\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.130675\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.129890\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.137992\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.225655\n",
      "[10] Test Loss: 0.0468, Accuracy: 98.52%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.161963\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.050325\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.101657\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.046831\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.095237\n",
      "[11] Test Loss: 0.0460, Accuracy: 98.61%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.072809\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.216486\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.080556\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.177321\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.059441\n",
      "[12] Test Loss: 0.0471, Accuracy: 98.52%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.254406\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.060651\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.281932\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.242518\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.207558\n",
      "[13] Test Loss: 0.0412, Accuracy: 98.80%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.105048\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.153025\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.103961\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.114933\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.113477\n",
      "[14] Test Loss: 0.0423, Accuracy: 98.75%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.076198\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.153039\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.263759\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.152107\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.229912\n",
      "[15] Test Loss: 0.0404, Accuracy: 98.76%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.059905\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.139136\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.168172\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.087679\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.074094\n",
      "[16] Test Loss: 0.0397, Accuracy: 98.82%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.185880\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.237392\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.120309\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.177898\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.135165\n",
      "[17] Test Loss: 0.0390, Accuracy: 98.85%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.221084\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.099147\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.200915\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.198344\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.091213\n",
      "[18] Test Loss: 0.0404, Accuracy: 98.76%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.214656\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.147548\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.155542\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.172592\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.082441\n",
      "[19] Test Loss: 0.0373, Accuracy: 98.88%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.078738\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.093994\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.759107\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.101053\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.220460\n",
      "[20] Test Loss: 0.0366, Accuracy: 98.97%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.211508\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.037639\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.060792\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.125424\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.045813\n",
      "[21] Test Loss: 0.0359, Accuracy: 98.90%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.149298\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.063464\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.195868\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.177151\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.049078\n",
      "[22] Test Loss: 0.0348, Accuracy: 99.01%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.176683\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.160817\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.023908\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.091269\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.075408\n",
      "[23] Test Loss: 0.0337, Accuracy: 99.06%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.050182\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.085619\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.235092\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.052457\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.199591\n",
      "[24] Test Loss: 0.0345, Accuracy: 99.06%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.127701\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.184897\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.129126\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.170008\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.314882\n",
      "[25] Test Loss: 0.0344, Accuracy: 98.94%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.015799\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.266462\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.204409\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.199853\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.022421\n",
      "[26] Test Loss: 0.0357, Accuracy: 98.86%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.064110\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.145476\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.112291\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.068100\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.067519\n",
      "[27] Test Loss: 0.0334, Accuracy: 98.99%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.110179\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.063640\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.028148\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.081247\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.019866\n",
      "[28] Test Loss: 0.0345, Accuracy: 98.95%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.057897\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.202611\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.120267\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.272384\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.208321\n",
      "[29] Test Loss: 0.0339, Accuracy: 98.99%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.072931\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.110464\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.048822\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.054016\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.232318\n",
      "[30] Test Loss: 0.0344, Accuracy: 98.98%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.100135\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.209329\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.108273\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.137781\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.045531\n",
      "[31] Test Loss: 0.0328, Accuracy: 99.03%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.023255\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.259366\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.037577\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.115032\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.217403\n",
      "[32] Test Loss: 0.0340, Accuracy: 98.96%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.095583\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.194401\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.027468\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.094275\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.095797\n",
      "[33] Test Loss: 0.0326, Accuracy: 99.04%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.080812\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.041507\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.085176\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.053212\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.102800\n",
      "[34] Test Loss: 0.0328, Accuracy: 99.07%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.217885\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.134605\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.079040\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.107517\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.191893\n",
      "[35] Test Loss: 0.0332, Accuracy: 99.01%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.095158\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.088967\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.137346\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.102294\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.140675\n",
      "[36] Test Loss: 0.0334, Accuracy: 98.99%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.036292\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.056137\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.024077\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.065889\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.069286\n",
      "[37] Test Loss: 0.0326, Accuracy: 99.03%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.184407\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.233097\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.022133\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.023312\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.087838\n",
      "[38] Test Loss: 0.0351, Accuracy: 99.01%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.056409\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.126246\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.040857\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.024543\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.129547\n",
      "[39] Test Loss: 0.0313, Accuracy: 99.02%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.044473\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.074190\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.127199\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.095758\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.177179\n",
      "[40] Test Loss: 0.0308, Accuracy: 99.03%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882fd90-bff1-4574-8bd7-d8ac7b2c17f6",
   "metadata": {},
   "source": [
    "* 정확도가 99%정도 됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
